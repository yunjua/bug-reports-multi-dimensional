07/18/2024 11:35:54 - INFO - transformers.tokenization_utils_base - loading file tokenizer.json

07/18/2024 11:35:54 - INFO - transformers.tokenization_utils_base - loading file added_tokens.json

07/18/2024 11:35:54 - INFO - transformers.tokenization_utils_base - loading file special_tokens_map.json

07/18/2024 11:35:54 - INFO - transformers.tokenization_utils_base - loading file tokenizer_config.json

07/18/2024 11:35:54 - WARNING - transformers.tokenization_utils_base - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

07/18/2024 11:35:54 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>

07/18/2024 11:35:54 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>

07/18/2024 11:35:54 - INFO - llamafactory.data.loader - Loading dataset env_train_modified.json...

07/18/2024 11:35:59 - INFO - transformers.configuration_utils - loading configuration file /root/autodl-fs/llama3-8b/Meta-Llama-3-8B/config.json

07/18/2024 11:35:59 - INFO - transformers.configuration_utils - Model config LlamaConfig {
  "_name_or_path": "/root/autodl-fs/llama3-8b/Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.42.3",
  "use_cache": true,
  "vocab_size": 128256
}


07/18/2024 11:35:59 - INFO - transformers.modeling_utils - loading weights file /root/autodl-fs/llama3-8b/Meta-Llama-3-8B/model.safetensors.index.json

07/18/2024 11:35:59 - INFO - transformers.modeling_utils - Instantiating LlamaForCausalLM model under default dtype torch.float16.

07/18/2024 11:35:59 - INFO - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}


07/18/2024 11:37:32 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing LlamaForCausalLM.


07/18/2024 11:37:32 - INFO - transformers.modeling_utils - All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/autodl-fs/llama3-8b/Meta-Llama-3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

07/18/2024 11:37:32 - INFO - transformers.generation.configuration_utils - loading configuration file /root/autodl-fs/llama3-8b/Meta-Llama-3-8B/generation_config.json

07/18/2024 11:37:32 - INFO - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_length": 4096,
  "temperature": 0.6,
  "top_p": 0.9
}


07/18/2024 11:37:32 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.

07/18/2024 11:37:32 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.

07/18/2024 11:37:32 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.

07/18/2024 11:37:32 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA

07/18/2024 11:37:32 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,v_proj,gate_proj,q_proj,k_proj,o_proj,up_proj

07/18/2024 11:37:33 - INFO - llamafactory.model.loader - trainable params: 20971520 || all params: 8051232768 || trainable%: 0.2605

07/18/2024 11:37:33 - INFO - transformers.trainer - Using auto half precision backend

07/18/2024 11:37:33 - INFO - transformers.trainer - ***** Running training *****

07/18/2024 11:37:33 - INFO - transformers.trainer -   Num examples = 1,297

07/18/2024 11:37:33 - INFO - transformers.trainer -   Num Epochs = 10

07/18/2024 11:37:33 - INFO - transformers.trainer -   Instantaneous batch size per device = 2

07/18/2024 11:37:33 - INFO - transformers.trainer -   Total train batch size (w. parallel, distributed & accumulation) = 16

07/18/2024 11:37:33 - INFO - transformers.trainer -   Gradient Accumulation steps = 8

07/18/2024 11:37:33 - INFO - transformers.trainer -   Total optimization steps = 810

07/18/2024 11:37:33 - INFO - transformers.trainer -   Number of trainable parameters = 20,971,520

07/18/2024 11:37:46 - INFO - llamafactory.extras.callbacks - {'loss': 3.7676, 'learning_rate': 2.0000e-04, 'epoch': 0.06, 'throughput': 1296.52}

07/18/2024 11:37:58 - INFO - llamafactory.extras.callbacks - {'loss': 3.1403, 'learning_rate': 1.9998e-04, 'epoch': 0.12, 'throughput': 1309.72}

07/18/2024 11:38:10 - INFO - llamafactory.extras.callbacks - {'loss': 2.1126, 'learning_rate': 1.9992e-04, 'epoch': 0.18, 'throughput': 1324.00}

07/18/2024 11:38:22 - INFO - llamafactory.extras.callbacks - {'loss': 2.0757, 'learning_rate': 1.9983e-04, 'epoch': 0.25, 'throughput': 1331.25}

07/18/2024 11:38:34 - INFO - llamafactory.extras.callbacks - {'loss': 1.6023, 'learning_rate': 1.9970e-04, 'epoch': 0.31, 'throughput': 1331.67}

07/18/2024 11:38:47 - INFO - llamafactory.extras.callbacks - {'loss': 1.5407, 'learning_rate': 1.9953e-04, 'epoch': 0.37, 'throughput': 1332.66}

07/18/2024 11:38:59 - INFO - llamafactory.extras.callbacks - {'loss': 1.5690, 'learning_rate': 1.9932e-04, 'epoch': 0.43, 'throughput': 1336.13}

07/18/2024 11:39:12 - INFO - llamafactory.extras.callbacks - {'loss': 1.5058, 'learning_rate': 1.9908e-04, 'epoch': 0.49, 'throughput': 1338.37}

07/18/2024 11:39:24 - INFO - llamafactory.extras.callbacks - {'loss': 1.3573, 'learning_rate': 1.9880e-04, 'epoch': 0.55, 'throughput': 1339.00}

07/18/2024 11:39:36 - INFO - llamafactory.extras.callbacks - {'loss': 1.5024, 'learning_rate': 1.9848e-04, 'epoch': 0.62, 'throughput': 1340.58}

07/18/2024 11:39:48 - INFO - llamafactory.extras.callbacks - {'loss': 1.3900, 'learning_rate': 1.9813e-04, 'epoch': 0.68, 'throughput': 1339.83}

07/18/2024 11:40:00 - INFO - llamafactory.extras.callbacks - {'loss': 1.7475, 'learning_rate': 1.9773e-04, 'epoch': 0.74, 'throughput': 1340.14}

07/18/2024 11:40:13 - INFO - llamafactory.extras.callbacks - {'loss': 1.4809, 'learning_rate': 1.9730e-04, 'epoch': 0.80, 'throughput': 1339.85}

07/18/2024 11:40:25 - INFO - llamafactory.extras.callbacks - {'loss': 1.6968, 'learning_rate': 1.9684e-04, 'epoch': 0.86, 'throughput': 1339.75}

07/18/2024 11:40:37 - INFO - llamafactory.extras.callbacks - {'loss': 1.5395, 'learning_rate': 1.9634e-04, 'epoch': 0.92, 'throughput': 1338.00}

07/18/2024 11:40:50 - INFO - llamafactory.extras.callbacks - {'loss': 1.5005, 'learning_rate': 1.9580e-04, 'epoch': 0.99, 'throughput': 1336.67}

07/18/2024 11:41:02 - INFO - llamafactory.extras.callbacks - {'loss': 1.2635, 'learning_rate': 1.9522e-04, 'epoch': 1.05, 'throughput': 1336.58}

07/18/2024 11:41:14 - INFO - llamafactory.extras.callbacks - {'loss': 1.4156, 'learning_rate': 1.9461e-04, 'epoch': 1.11, 'throughput': 1336.04}

07/18/2024 11:41:27 - INFO - llamafactory.extras.callbacks - {'loss': 1.2454, 'learning_rate': 1.9397e-04, 'epoch': 1.17, 'throughput': 1335.85}

07/18/2024 11:41:39 - INFO - llamafactory.extras.callbacks - {'loss': 1.4848, 'learning_rate': 1.9329e-04, 'epoch': 1.23, 'throughput': 1336.28}

07/18/2024 11:41:39 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_env/checkpoint-100

07/18/2024 11:41:40 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-100/tokenizer_config.json

07/18/2024 11:41:40 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-100/special_tokens_map.json

07/18/2024 11:41:53 - INFO - llamafactory.extras.callbacks - {'loss': 1.2342, 'learning_rate': 1.9257e-04, 'epoch': 1.29, 'throughput': 1328.84}

07/18/2024 11:42:05 - INFO - llamafactory.extras.callbacks - {'loss': 1.1470, 'learning_rate': 1.9182e-04, 'epoch': 1.36, 'throughput': 1329.87}

07/18/2024 11:42:18 - INFO - llamafactory.extras.callbacks - {'loss': 1.1413, 'learning_rate': 1.9104e-04, 'epoch': 1.42, 'throughput': 1330.64}

07/18/2024 11:42:31 - INFO - llamafactory.extras.callbacks - {'loss': 1.0383, 'learning_rate': 1.9022e-04, 'epoch': 1.48, 'throughput': 1330.78}

07/18/2024 11:42:43 - INFO - llamafactory.extras.callbacks - {'loss': 1.5206, 'learning_rate': 1.8936e-04, 'epoch': 1.54, 'throughput': 1331.15}

07/18/2024 11:42:55 - INFO - llamafactory.extras.callbacks - {'loss': 1.3728, 'learning_rate': 1.8848e-04, 'epoch': 1.60, 'throughput': 1330.94}

07/18/2024 11:43:07 - INFO - llamafactory.extras.callbacks - {'loss': 1.1385, 'learning_rate': 1.8756e-04, 'epoch': 1.66, 'throughput': 1332.33}

07/18/2024 11:43:19 - INFO - llamafactory.extras.callbacks - {'loss': 1.4142, 'learning_rate': 1.8660e-04, 'epoch': 1.73, 'throughput': 1332.11}

07/18/2024 11:43:31 - INFO - llamafactory.extras.callbacks - {'loss': 1.4823, 'learning_rate': 1.8562e-04, 'epoch': 1.79, 'throughput': 1332.76}

07/18/2024 11:43:44 - INFO - llamafactory.extras.callbacks - {'loss': 1.3280, 'learning_rate': 1.8460e-04, 'epoch': 1.85, 'throughput': 1332.97}

07/18/2024 11:43:56 - INFO - llamafactory.extras.callbacks - {'loss': 1.2759, 'learning_rate': 1.8355e-04, 'epoch': 1.91, 'throughput': 1332.88}

07/18/2024 11:44:08 - INFO - llamafactory.extras.callbacks - {'loss': 1.3493, 'learning_rate': 1.8247e-04, 'epoch': 1.97, 'throughput': 1332.95}

07/18/2024 11:44:20 - INFO - llamafactory.extras.callbacks - {'loss': 1.2435, 'learning_rate': 1.8136e-04, 'epoch': 2.03, 'throughput': 1333.55}

07/18/2024 11:44:32 - INFO - llamafactory.extras.callbacks - {'loss': 1.0363, 'learning_rate': 1.8021e-04, 'epoch': 2.10, 'throughput': 1333.65}

07/18/2024 11:44:44 - INFO - llamafactory.extras.callbacks - {'loss': 1.1753, 'learning_rate': 1.7904e-04, 'epoch': 2.16, 'throughput': 1334.04}

07/18/2024 11:44:56 - INFO - llamafactory.extras.callbacks - {'loss': 1.0811, 'learning_rate': 1.7784e-04, 'epoch': 2.22, 'throughput': 1334.52}

07/18/2024 11:45:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.9622, 'learning_rate': 1.7660e-04, 'epoch': 2.28, 'throughput': 1334.71}

07/18/2024 11:45:21 - INFO - llamafactory.extras.callbacks - {'loss': 1.0557, 'learning_rate': 1.7534e-04, 'epoch': 2.34, 'throughput': 1335.22}

07/18/2024 11:45:33 - INFO - llamafactory.extras.callbacks - {'loss': 1.1005, 'learning_rate': 1.7405e-04, 'epoch': 2.40, 'throughput': 1335.68}

07/18/2024 11:45:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.8412, 'learning_rate': 1.7274e-04, 'epoch': 2.47, 'throughput': 1335.61}

07/18/2024 11:45:45 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_env/checkpoint-200

07/18/2024 11:45:46 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-200/tokenizer_config.json

07/18/2024 11:45:46 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-200/special_tokens_map.json

07/18/2024 11:45:58 - INFO - llamafactory.extras.callbacks - {'loss': 1.2436, 'learning_rate': 1.7139e-04, 'epoch': 2.53, 'throughput': 1332.19}

07/18/2024 11:46:11 - INFO - llamafactory.extras.callbacks - {'loss': 1.0555, 'learning_rate': 1.7002e-04, 'epoch': 2.59, 'throughput': 1332.25}

07/18/2024 11:46:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.9757, 'learning_rate': 1.6862e-04, 'epoch': 2.65, 'throughput': 1332.60}

07/18/2024 11:46:35 - INFO - llamafactory.extras.callbacks - {'loss': 1.0572, 'learning_rate': 1.6720e-04, 'epoch': 2.71, 'throughput': 1332.46}

07/18/2024 11:46:47 - INFO - llamafactory.extras.callbacks - {'loss': 1.1164, 'learning_rate': 1.6575e-04, 'epoch': 2.77, 'throughput': 1332.74}

07/18/2024 11:47:00 - INFO - llamafactory.extras.callbacks - {'loss': 1.0167, 'learning_rate': 1.6428e-04, 'epoch': 2.84, 'throughput': 1332.89}

07/18/2024 11:47:12 - INFO - llamafactory.extras.callbacks - {'loss': 1.0693, 'learning_rate': 1.6278e-04, 'epoch': 2.90, 'throughput': 1332.82}

07/18/2024 11:47:24 - INFO - llamafactory.extras.callbacks - {'loss': 1.0314, 'learning_rate': 1.6126e-04, 'epoch': 2.96, 'throughput': 1333.05}

07/18/2024 11:47:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.9950, 'learning_rate': 1.5972e-04, 'epoch': 3.02, 'throughput': 1333.11}

07/18/2024 11:47:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.9397, 'learning_rate': 1.5815e-04, 'epoch': 3.08, 'throughput': 1333.45}

07/18/2024 11:48:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.9470, 'learning_rate': 1.5656e-04, 'epoch': 3.14, 'throughput': 1333.70}

07/18/2024 11:48:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.8054, 'learning_rate': 1.5495e-04, 'epoch': 3.20, 'throughput': 1333.21}

07/18/2024 11:48:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.9227, 'learning_rate': 1.5332e-04, 'epoch': 3.27, 'throughput': 1333.43}

07/18/2024 11:48:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.9278, 'learning_rate': 1.5167e-04, 'epoch': 3.33, 'throughput': 1333.46}

07/18/2024 11:48:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.8506, 'learning_rate': 1.5000e-04, 'epoch': 3.39, 'throughput': 1333.37}

07/18/2024 11:49:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.9006, 'learning_rate': 1.4831e-04, 'epoch': 3.45, 'throughput': 1333.69}

07/18/2024 11:49:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.8701, 'learning_rate': 1.4660e-04, 'epoch': 3.51, 'throughput': 1333.90}

07/18/2024 11:49:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.8792, 'learning_rate': 1.4488e-04, 'epoch': 3.57, 'throughput': 1333.77}

07/18/2024 11:49:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.9528, 'learning_rate': 1.4314e-04, 'epoch': 3.64, 'throughput': 1333.77}

07/18/2024 11:49:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.7985, 'learning_rate': 1.4138e-04, 'epoch': 3.70, 'throughput': 1334.23}

07/18/2024 11:49:51 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_env/checkpoint-300

07/18/2024 11:49:51 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-300/tokenizer_config.json

07/18/2024 11:49:51 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-300/special_tokens_map.json

07/18/2024 11:50:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.7462, 'learning_rate': 1.3961e-04, 'epoch': 3.76, 'throughput': 1331.36}

07/18/2024 11:50:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.9950, 'learning_rate': 1.3782e-04, 'epoch': 3.82, 'throughput': 1331.92}

07/18/2024 11:50:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.9403, 'learning_rate': 1.3602e-04, 'epoch': 3.88, 'throughput': 1332.50}

07/18/2024 11:50:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.7970, 'learning_rate': 1.3420e-04, 'epoch': 3.94, 'throughput': 1332.65}

07/18/2024 11:50:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.9212, 'learning_rate': 1.3237e-04, 'epoch': 4.01, 'throughput': 1332.81}

07/18/2024 11:51:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.7810, 'learning_rate': 1.3053e-04, 'epoch': 4.07, 'throughput': 1333.01}

07/18/2024 11:51:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.6880, 'learning_rate': 1.2868e-04, 'epoch': 4.13, 'throughput': 1333.34}

07/18/2024 11:51:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.8107, 'learning_rate': 1.2682e-04, 'epoch': 4.19, 'throughput': 1333.58}

07/18/2024 11:51:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.7283, 'learning_rate': 1.2494e-04, 'epoch': 4.25, 'throughput': 1333.63}

07/18/2024 11:51:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.8280, 'learning_rate': 1.2306e-04, 'epoch': 4.31, 'throughput': 1333.86}

07/18/2024 11:52:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.7540, 'learning_rate': 1.2117e-04, 'epoch': 4.38, 'throughput': 1334.04}

07/18/2024 11:52:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.7655, 'learning_rate': 1.1927e-04, 'epoch': 4.44, 'throughput': 1334.21}

07/18/2024 11:52:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.8466, 'learning_rate': 1.1736e-04, 'epoch': 4.50, 'throughput': 1334.42}

07/18/2024 11:52:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.7064, 'learning_rate': 1.1545e-04, 'epoch': 4.56, 'throughput': 1334.46}

07/18/2024 11:52:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.7172, 'learning_rate': 1.1353e-04, 'epoch': 4.62, 'throughput': 1334.49}

07/18/2024 11:53:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.7275, 'learning_rate': 1.1161e-04, 'epoch': 4.68, 'throughput': 1334.59}

07/18/2024 11:53:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.7315, 'learning_rate': 1.0968e-04, 'epoch': 4.75, 'throughput': 1334.81}

07/18/2024 11:53:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.7893, 'learning_rate': 1.0775e-04, 'epoch': 4.81, 'throughput': 1335.09}

07/18/2024 11:53:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.8607, 'learning_rate': 1.0581e-04, 'epoch': 4.87, 'throughput': 1335.16}

07/18/2024 11:53:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.7047, 'learning_rate': 1.0388e-04, 'epoch': 4.93, 'throughput': 1335.24}

07/18/2024 11:53:56 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_env/checkpoint-400

07/18/2024 11:53:57 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-400/tokenizer_config.json

07/18/2024 11:53:57 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-400/special_tokens_map.json

07/18/2024 11:54:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.7397, 'learning_rate': 1.0194e-04, 'epoch': 4.99, 'throughput': 1333.08}

07/18/2024 11:54:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.7398, 'learning_rate': 1.0000e-04, 'epoch': 5.05, 'throughput': 1333.33}

07/18/2024 11:54:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.6029, 'learning_rate': 9.8061e-05, 'epoch': 5.12, 'throughput': 1333.42}

07/18/2024 11:54:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.6786, 'learning_rate': 9.6122e-05, 'epoch': 5.18, 'throughput': 1333.63}

07/18/2024 11:54:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.6769, 'learning_rate': 9.4186e-05, 'epoch': 5.24, 'throughput': 1333.63}

07/18/2024 11:55:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.6728, 'learning_rate': 9.2251e-05, 'epoch': 5.30, 'throughput': 1333.82}

07/18/2024 11:55:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.6743, 'learning_rate': 9.0319e-05, 'epoch': 5.36, 'throughput': 1333.77}

07/18/2024 11:55:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.7131, 'learning_rate': 8.8391e-05, 'epoch': 5.42, 'throughput': 1333.75}

07/18/2024 11:55:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.6369, 'learning_rate': 8.6467e-05, 'epoch': 5.49, 'throughput': 1333.94}

07/18/2024 11:55:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.7012, 'learning_rate': 8.4548e-05, 'epoch': 5.55, 'throughput': 1333.89}

07/18/2024 11:56:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.5946, 'learning_rate': 8.2635e-05, 'epoch': 5.61, 'throughput': 1334.13}

07/18/2024 11:56:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.7081, 'learning_rate': 8.0729e-05, 'epoch': 5.67, 'throughput': 1334.11}

07/18/2024 11:56:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.6956, 'learning_rate': 7.8830e-05, 'epoch': 5.73, 'throughput': 1334.11}

07/18/2024 11:56:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.8215, 'learning_rate': 7.6938e-05, 'epoch': 5.79, 'throughput': 1334.11}

07/18/2024 11:57:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.6253, 'learning_rate': 7.5056e-05, 'epoch': 5.86, 'throughput': 1334.17}

07/18/2024 11:57:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.6943, 'learning_rate': 7.3183e-05, 'epoch': 5.92, 'throughput': 1334.20}

07/18/2024 11:57:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.6804, 'learning_rate': 7.1320e-05, 'epoch': 5.98, 'throughput': 1334.54}

07/18/2024 11:57:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.6397, 'learning_rate': 6.9467e-05, 'epoch': 6.04, 'throughput': 1334.46}

07/18/2024 11:57:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.6501, 'learning_rate': 6.7627e-05, 'epoch': 6.10, 'throughput': 1334.54}

07/18/2024 11:58:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.5781, 'learning_rate': 6.5798e-05, 'epoch': 6.16, 'throughput': 1334.74}

07/18/2024 11:58:02 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_env/checkpoint-500

07/18/2024 11:58:02 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-500/tokenizer_config.json

07/18/2024 11:58:02 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-500/special_tokens_map.json

07/18/2024 11:58:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.5733, 'learning_rate': 6.3982e-05, 'epoch': 6.22, 'throughput': 1333.53}

07/18/2024 11:58:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.7352, 'learning_rate': 6.2180e-05, 'epoch': 6.29, 'throughput': 1333.75}

07/18/2024 11:58:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.6575, 'learning_rate': 6.0392e-05, 'epoch': 6.35, 'throughput': 1333.92}

07/18/2024 11:58:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.6727, 'learning_rate': 5.8619e-05, 'epoch': 6.41, 'throughput': 1334.16}

07/18/2024 11:59:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.6364, 'learning_rate': 5.6861e-05, 'epoch': 6.47, 'throughput': 1334.27}

07/18/2024 11:59:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.6592, 'learning_rate': 5.5120e-05, 'epoch': 6.53, 'throughput': 1334.31}

07/18/2024 11:59:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.6490, 'learning_rate': 5.3396e-05, 'epoch': 6.59, 'throughput': 1334.35}

07/18/2024 11:59:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.6642, 'learning_rate': 5.1689e-05, 'epoch': 6.66, 'throughput': 1334.30}

07/18/2024 11:59:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.5907, 'learning_rate': 5.0000e-05, 'epoch': 6.72, 'throughput': 1334.31}

07/18/2024 12:00:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.7011, 'learning_rate': 4.8330e-05, 'epoch': 6.78, 'throughput': 1334.38}

07/18/2024 12:00:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.6342, 'learning_rate': 4.6680e-05, 'epoch': 6.84, 'throughput': 1334.44}

07/18/2024 12:00:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.6554, 'learning_rate': 4.5049e-05, 'epoch': 6.90, 'throughput': 1334.65}

07/18/2024 12:00:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.6029, 'learning_rate': 4.3439e-05, 'epoch': 6.96, 'throughput': 1334.69}

07/18/2024 12:00:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.5653, 'learning_rate': 4.1851e-05, 'epoch': 7.03, 'throughput': 1334.54}

07/18/2024 12:01:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.6209, 'learning_rate': 4.0284e-05, 'epoch': 7.09, 'throughput': 1334.61}

07/18/2024 12:01:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.5888, 'learning_rate': 3.8740e-05, 'epoch': 7.15, 'throughput': 1334.61}

07/18/2024 12:01:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.6168, 'learning_rate': 3.7219e-05, 'epoch': 7.21, 'throughput': 1334.57}

07/18/2024 12:01:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.6791, 'learning_rate': 3.5721e-05, 'epoch': 7.27, 'throughput': 1334.62}

07/18/2024 12:01:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.6118, 'learning_rate': 3.4248e-05, 'epoch': 7.33, 'throughput': 1334.50}

07/18/2024 12:02:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.6418, 'learning_rate': 3.2799e-05, 'epoch': 7.40, 'throughput': 1334.51}

07/18/2024 12:02:08 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_env/checkpoint-600

07/18/2024 12:02:09 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-600/tokenizer_config.json

07/18/2024 12:02:09 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-600/special_tokens_map.json

07/18/2024 12:02:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.6582, 'learning_rate': 3.1376e-05, 'epoch': 7.46, 'throughput': 1333.22}

07/18/2024 12:02:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.5367, 'learning_rate': 2.9978e-05, 'epoch': 7.52, 'throughput': 1333.42}

07/18/2024 12:02:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.6280, 'learning_rate': 2.8607e-05, 'epoch': 7.58, 'throughput': 1333.57}

07/18/2024 12:02:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.6266, 'learning_rate': 2.7263e-05, 'epoch': 7.64, 'throughput': 1333.62}

07/18/2024 12:03:11 - INFO - llamafactory.extras.callbacks - {'loss': 0.6361, 'learning_rate': 2.5946e-05, 'epoch': 7.70, 'throughput': 1333.71}

07/18/2024 12:03:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.6404, 'learning_rate': 2.4656e-05, 'epoch': 7.77, 'throughput': 1333.93}

07/18/2024 12:03:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.5112, 'learning_rate': 2.3396e-05, 'epoch': 7.83, 'throughput': 1334.00}

07/18/2024 12:03:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.6340, 'learning_rate': 2.2164e-05, 'epoch': 7.89, 'throughput': 1334.19}

07/18/2024 12:04:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.6909, 'learning_rate': 2.0961e-05, 'epoch': 7.95, 'throughput': 1334.19}

07/18/2024 12:04:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.6791, 'learning_rate': 1.9788e-05, 'epoch': 8.01, 'throughput': 1334.21}

07/18/2024 12:04:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.5657, 'learning_rate': 1.8645e-05, 'epoch': 8.07, 'throughput': 1334.27}

07/18/2024 12:04:36 - INFO - llamafactory.extras.callbacks - {'loss': 0.7342, 'learning_rate': 1.7532e-05, 'epoch': 8.14, 'throughput': 1334.29}

07/18/2024 12:04:48 - INFO - llamafactory.extras.callbacks - {'loss': 0.7377, 'learning_rate': 1.6451e-05, 'epoch': 8.20, 'throughput': 1334.40}

07/18/2024 12:05:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.5860, 'learning_rate': 1.5401e-05, 'epoch': 8.26, 'throughput': 1334.59}

07/18/2024 12:05:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.5556, 'learning_rate': 1.4383e-05, 'epoch': 8.32, 'throughput': 1334.67}

07/18/2024 12:05:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.6970, 'learning_rate': 1.3397e-05, 'epoch': 8.38, 'throughput': 1334.66}

07/18/2024 12:05:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.6024, 'learning_rate': 1.2444e-05, 'epoch': 8.44, 'throughput': 1334.79}

07/18/2024 12:05:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.5984, 'learning_rate': 1.1524e-05, 'epoch': 8.51, 'throughput': 1334.72}

07/18/2024 12:06:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.5595, 'learning_rate': 1.0637e-05, 'epoch': 8.57, 'throughput': 1334.75}

07/18/2024 12:06:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.5494, 'learning_rate': 9.7833e-06, 'epoch': 8.63, 'throughput': 1334.83}

07/18/2024 12:06:14 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_env/checkpoint-700

07/18/2024 12:06:15 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-700/tokenizer_config.json

07/18/2024 12:06:15 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-700/special_tokens_map.json

07/18/2024 12:06:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.6850, 'learning_rate': 8.9637e-06, 'epoch': 8.69, 'throughput': 1333.68}

07/18/2024 12:06:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.5398, 'learning_rate': 8.1784e-06, 'epoch': 8.75, 'throughput': 1333.74}

07/18/2024 12:06:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.5803, 'learning_rate': 7.4276e-06, 'epoch': 8.81, 'throughput': 1333.73}

07/18/2024 12:07:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.5152, 'learning_rate': 6.7116e-06, 'epoch': 8.88, 'throughput': 1333.86}

07/18/2024 12:07:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.6154, 'learning_rate': 6.0307e-06, 'epoch': 8.94, 'throughput': 1334.06}

07/18/2024 12:07:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.5963, 'learning_rate': 5.3852e-06, 'epoch': 9.00, 'throughput': 1334.22}

07/18/2024 12:07:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.5874, 'learning_rate': 4.7752e-06, 'epoch': 9.06, 'throughput': 1334.31}

07/18/2024 12:07:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.6685, 'learning_rate': 4.2010e-06, 'epoch': 9.12, 'throughput': 1334.19}

07/18/2024 12:08:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.6286, 'learning_rate': 3.6629e-06, 'epoch': 9.18, 'throughput': 1334.27}

07/18/2024 12:08:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.6352, 'learning_rate': 3.1610e-06, 'epoch': 9.24, 'throughput': 1334.26}

07/18/2024 12:08:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.6471, 'learning_rate': 2.6955e-06, 'epoch': 9.31, 'throughput': 1334.36}

07/18/2024 12:08:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.6083, 'learning_rate': 2.2666e-06, 'epoch': 9.37, 'throughput': 1334.40}

07/18/2024 12:08:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.5455, 'learning_rate': 1.8745e-06, 'epoch': 9.43, 'throughput': 1334.42}

07/18/2024 12:09:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.6686, 'learning_rate': 1.5192e-06, 'epoch': 9.49, 'throughput': 1334.46}

07/18/2024 12:09:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.6383, 'learning_rate': 1.2010e-06, 'epoch': 9.55, 'throughput': 1334.63}

07/18/2024 12:09:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.5718, 'learning_rate': 9.1996e-07, 'epoch': 9.61, 'throughput': 1334.71}

07/18/2024 12:09:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.5969, 'learning_rate': 6.7616e-07, 'epoch': 9.68, 'throughput': 1334.76}

07/18/2024 12:09:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.6190, 'learning_rate': 4.6972e-07, 'epoch': 9.74, 'throughput': 1334.83}

07/18/2024 12:10:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.5043, 'learning_rate': 3.0071e-07, 'epoch': 9.80, 'throughput': 1334.88}

07/18/2024 12:10:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.6157, 'learning_rate': 1.6918e-07, 'epoch': 9.86, 'throughput': 1335.04}

07/18/2024 12:10:20 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_env/checkpoint-800

07/18/2024 12:10:20 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-800/tokenizer_config.json

07/18/2024 12:10:20 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-800/special_tokens_map.json

07/18/2024 12:10:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.6502, 'learning_rate': 7.5205e-08, 'epoch': 9.92, 'throughput': 1334.13}

07/18/2024 12:10:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.6131, 'learning_rate': 1.8803e-08, 'epoch': 9.98, 'throughput': 1334.13}

07/18/2024 12:10:46 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_env/checkpoint-810

07/18/2024 12:10:46 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-810/tokenizer_config.json

07/18/2024 12:10:46 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_env/checkpoint-810/special_tokens_map.json

07/18/2024 12:10:47 - INFO - transformers.trainer - 

Training completed. Do not forget to share your model on huggingface.co/models =)



07/18/2024 12:10:47 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_env

07/18/2024 12:10:48 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_env/tokenizer_config.json

07/18/2024 12:10:48 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_env/special_tokens_map.json

07/18/2024 12:10:48 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.

07/18/2024 12:10:48 - INFO - transformers.modelcard - Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}

