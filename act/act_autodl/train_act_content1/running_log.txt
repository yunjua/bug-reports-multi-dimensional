07/23/2024 19:50:35 - INFO - transformers.tokenization_utils_base - loading file tokenizer.json

07/23/2024 19:50:35 - INFO - transformers.tokenization_utils_base - loading file added_tokens.json

07/23/2024 19:50:35 - INFO - transformers.tokenization_utils_base - loading file special_tokens_map.json

07/23/2024 19:50:35 - INFO - transformers.tokenization_utils_base - loading file tokenizer_config.json

07/23/2024 19:50:35 - WARNING - transformers.tokenization_utils_base - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

07/23/2024 19:50:35 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>

07/23/2024 19:50:35 - INFO - llamafactory.data.template - Add pad token: <|eot_id|>

07/23/2024 19:50:35 - INFO - llamafactory.data.loader - Loading dataset act_train_modified.json...

07/23/2024 19:50:40 - INFO - transformers.configuration_utils - loading configuration file /root/autodl-fs/llama3-8b/Meta-Llama-3-8B/config.json

07/23/2024 19:50:40 - INFO - transformers.configuration_utils - Model config LlamaConfig {
  "_name_or_path": "/root/autodl-fs/llama3-8b/Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.42.3",
  "use_cache": true,
  "vocab_size": 128256
}


07/23/2024 19:50:40 - INFO - transformers.modeling_utils - loading weights file /root/autodl-fs/llama3-8b/Meta-Llama-3-8B/model.safetensors.index.json

07/23/2024 19:50:40 - INFO - transformers.modeling_utils - Instantiating LlamaForCausalLM model under default dtype torch.float16.

07/23/2024 19:50:40 - INFO - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}


07/23/2024 19:52:16 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing LlamaForCausalLM.


07/23/2024 19:52:16 - INFO - transformers.modeling_utils - All the weights of LlamaForCausalLM were initialized from the model checkpoint at /root/autodl-fs/llama3-8b/Meta-Llama-3-8B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.

07/23/2024 19:52:16 - INFO - transformers.generation.configuration_utils - loading configuration file /root/autodl-fs/llama3-8b/Meta-Llama-3-8B/generation_config.json

07/23/2024 19:52:16 - INFO - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_length": 4096,
  "temperature": 0.6,
  "top_p": 0.9
}


07/23/2024 19:52:16 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.

07/23/2024 19:52:16 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.

07/23/2024 19:52:16 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.

07/23/2024 19:52:16 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA

07/23/2024 19:52:16 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,up_proj,k_proj,q_proj,o_proj,gate_proj,v_proj

07/23/2024 19:52:17 - INFO - llamafactory.model.loader - trainable params: 20971520 || all params: 8051232768 || trainable%: 0.2605

07/23/2024 19:52:17 - INFO - transformers.trainer - Using auto half precision backend

07/23/2024 19:52:17 - INFO - transformers.trainer - ***** Running training *****

07/23/2024 19:52:17 - INFO - transformers.trainer -   Num examples = 1,663

07/23/2024 19:52:17 - INFO - transformers.trainer -   Num Epochs = 10

07/23/2024 19:52:17 - INFO - transformers.trainer -   Instantaneous batch size per device = 2

07/23/2024 19:52:17 - INFO - transformers.trainer -   Total train batch size (w. parallel, distributed & accumulation) = 16

07/23/2024 19:52:17 - INFO - transformers.trainer -   Gradient Accumulation steps = 8

07/23/2024 19:52:17 - INFO - transformers.trainer -   Total optimization steps = 1,040

07/23/2024 19:52:17 - INFO - transformers.trainer -   Number of trainable parameters = 20,971,520

07/23/2024 19:52:29 - INFO - llamafactory.extras.callbacks - {'loss': 6.1012, 'learning_rate': 2.0000e-04, 'epoch': 0.05, 'throughput': 1321.66}

07/23/2024 19:52:41 - INFO - llamafactory.extras.callbacks - {'loss': 5.4488, 'learning_rate': 1.9999e-04, 'epoch': 0.10, 'throughput': 1335.65}

07/23/2024 19:52:53 - INFO - llamafactory.extras.callbacks - {'loss': 3.4325, 'learning_rate': 1.9995e-04, 'epoch': 0.14, 'throughput': 1338.29}

07/23/2024 19:53:04 - INFO - llamafactory.extras.callbacks - {'loss': 3.3675, 'learning_rate': 1.9990e-04, 'epoch': 0.19, 'throughput': 1346.84}

07/23/2024 19:53:16 - INFO - llamafactory.extras.callbacks - {'loss': 3.1680, 'learning_rate': 1.9982e-04, 'epoch': 0.24, 'throughput': 1349.28}

07/23/2024 19:53:28 - INFO - llamafactory.extras.callbacks - {'loss': 2.8818, 'learning_rate': 1.9971e-04, 'epoch': 0.29, 'throughput': 1345.53}

07/23/2024 19:53:40 - INFO - llamafactory.extras.callbacks - {'loss': 2.9270, 'learning_rate': 1.9959e-04, 'epoch': 0.34, 'throughput': 1348.82}

07/23/2024 19:53:52 - INFO - llamafactory.extras.callbacks - {'loss': 2.7349, 'learning_rate': 1.9944e-04, 'epoch': 0.38, 'throughput': 1348.30}

07/23/2024 19:54:03 - INFO - llamafactory.extras.callbacks - {'loss': 2.5006, 'learning_rate': 1.9927e-04, 'epoch': 0.43, 'throughput': 1353.76}

07/23/2024 19:54:15 - INFO - llamafactory.extras.callbacks - {'loss': 2.6887, 'learning_rate': 1.9908e-04, 'epoch': 0.48, 'throughput': 1352.39}

07/23/2024 19:54:26 - INFO - llamafactory.extras.callbacks - {'loss': 3.0217, 'learning_rate': 1.9886e-04, 'epoch': 0.53, 'throughput': 1353.71}

07/23/2024 19:54:38 - INFO - llamafactory.extras.callbacks - {'loss': 2.9889, 'learning_rate': 1.9862e-04, 'epoch': 0.58, 'throughput': 1354.54}

07/23/2024 19:54:50 - INFO - llamafactory.extras.callbacks - {'loss': 2.8834, 'learning_rate': 1.9836e-04, 'epoch': 0.62, 'throughput': 1353.92}

07/23/2024 19:55:02 - INFO - llamafactory.extras.callbacks - {'loss': 2.5814, 'learning_rate': 1.9808e-04, 'epoch': 0.67, 'throughput': 1355.09}

07/23/2024 19:55:13 - INFO - llamafactory.extras.callbacks - {'loss': 2.8315, 'learning_rate': 1.9777e-04, 'epoch': 0.72, 'throughput': 1355.47}

07/23/2024 19:55:25 - INFO - llamafactory.extras.callbacks - {'loss': 2.4911, 'learning_rate': 1.9744e-04, 'epoch': 0.77, 'throughput': 1354.67}

07/23/2024 19:55:37 - INFO - llamafactory.extras.callbacks - {'loss': 2.6114, 'learning_rate': 1.9709e-04, 'epoch': 0.82, 'throughput': 1354.87}

07/23/2024 19:55:48 - INFO - llamafactory.extras.callbacks - {'loss': 2.8779, 'learning_rate': 1.9672e-04, 'epoch': 0.87, 'throughput': 1354.91}

07/23/2024 19:56:00 - INFO - llamafactory.extras.callbacks - {'loss': 2.6759, 'learning_rate': 1.9633e-04, 'epoch': 0.91, 'throughput': 1353.81}

07/23/2024 19:56:12 - INFO - llamafactory.extras.callbacks - {'loss': 2.5850, 'learning_rate': 1.9591e-04, 'epoch': 0.96, 'throughput': 1353.64}

07/23/2024 19:56:12 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_act_content1/checkpoint-100

07/23/2024 19:56:13 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-100/tokenizer_config.json

07/23/2024 19:56:13 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-100/special_tokens_map.json

07/23/2024 19:56:26 - INFO - llamafactory.extras.callbacks - {'loss': 2.4807, 'learning_rate': 1.9547e-04, 'epoch': 1.01, 'throughput': 1344.18}

07/23/2024 19:56:38 - INFO - llamafactory.extras.callbacks - {'loss': 2.2595, 'learning_rate': 1.9501e-04, 'epoch': 1.06, 'throughput': 1343.58}

07/23/2024 19:56:49 - INFO - llamafactory.extras.callbacks - {'loss': 2.0083, 'learning_rate': 1.9453e-04, 'epoch': 1.11, 'throughput': 1345.01}

07/23/2024 19:57:01 - INFO - llamafactory.extras.callbacks - {'loss': 2.1175, 'learning_rate': 1.9403e-04, 'epoch': 1.15, 'throughput': 1345.17}

07/23/2024 19:57:13 - INFO - llamafactory.extras.callbacks - {'loss': 2.2395, 'learning_rate': 1.9350e-04, 'epoch': 1.20, 'throughput': 1345.11}

07/23/2024 19:57:25 - INFO - llamafactory.extras.callbacks - {'loss': 2.0872, 'learning_rate': 1.9296e-04, 'epoch': 1.25, 'throughput': 1345.57}

07/23/2024 19:57:36 - INFO - llamafactory.extras.callbacks - {'loss': 2.2489, 'learning_rate': 1.9239e-04, 'epoch': 1.30, 'throughput': 1344.40}

07/23/2024 19:57:49 - INFO - llamafactory.extras.callbacks - {'loss': 2.1239, 'learning_rate': 1.9180e-04, 'epoch': 1.35, 'throughput': 1343.87}

07/23/2024 19:58:01 - INFO - llamafactory.extras.callbacks - {'loss': 2.2678, 'learning_rate': 1.9119e-04, 'epoch': 1.39, 'throughput': 1344.57}

07/23/2024 19:58:12 - INFO - llamafactory.extras.callbacks - {'loss': 2.0943, 'learning_rate': 1.9056e-04, 'epoch': 1.44, 'throughput': 1344.57}

07/23/2024 19:58:24 - INFO - llamafactory.extras.callbacks - {'loss': 2.3213, 'learning_rate': 1.8991e-04, 'epoch': 1.49, 'throughput': 1344.99}

07/23/2024 19:58:36 - INFO - llamafactory.extras.callbacks - {'loss': 2.1883, 'learning_rate': 1.8924e-04, 'epoch': 1.54, 'throughput': 1345.85}

07/23/2024 19:58:48 - INFO - llamafactory.extras.callbacks - {'loss': 1.9729, 'learning_rate': 1.8855e-04, 'epoch': 1.59, 'throughput': 1345.02}

07/23/2024 19:59:00 - INFO - llamafactory.extras.callbacks - {'loss': 2.3112, 'learning_rate': 1.8783e-04, 'epoch': 1.63, 'throughput': 1345.09}

07/23/2024 19:59:11 - INFO - llamafactory.extras.callbacks - {'loss': 2.1777, 'learning_rate': 1.8710e-04, 'epoch': 1.68, 'throughput': 1345.36}

07/23/2024 19:59:23 - INFO - llamafactory.extras.callbacks - {'loss': 2.2039, 'learning_rate': 1.8635e-04, 'epoch': 1.73, 'throughput': 1345.51}

07/23/2024 19:59:35 - INFO - llamafactory.extras.callbacks - {'loss': 2.2825, 'learning_rate': 1.8558e-04, 'epoch': 1.78, 'throughput': 1345.24}

07/23/2024 19:59:47 - INFO - llamafactory.extras.callbacks - {'loss': 2.2010, 'learning_rate': 1.8479e-04, 'epoch': 1.83, 'throughput': 1345.36}

07/23/2024 19:59:58 - INFO - llamafactory.extras.callbacks - {'loss': 2.1104, 'learning_rate': 1.8398e-04, 'epoch': 1.88, 'throughput': 1345.08}

07/23/2024 20:00:10 - INFO - llamafactory.extras.callbacks - {'loss': 2.0925, 'learning_rate': 1.8315e-04, 'epoch': 1.92, 'throughput': 1345.68}

07/23/2024 20:00:10 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_act_content1/checkpoint-200

07/23/2024 20:00:11 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-200/tokenizer_config.json

07/23/2024 20:00:11 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-200/special_tokens_map.json

07/23/2024 20:00:23 - INFO - llamafactory.extras.callbacks - {'loss': 2.4435, 'learning_rate': 1.8230e-04, 'epoch': 1.97, 'throughput': 1341.71}

07/23/2024 20:00:35 - INFO - llamafactory.extras.callbacks - {'loss': 1.9823, 'learning_rate': 1.8143e-04, 'epoch': 2.02, 'throughput': 1341.76}

07/23/2024 20:00:48 - INFO - llamafactory.extras.callbacks - {'loss': 1.4165, 'learning_rate': 1.8055e-04, 'epoch': 2.07, 'throughput': 1341.46}

07/23/2024 20:00:59 - INFO - llamafactory.extras.callbacks - {'loss': 1.5783, 'learning_rate': 1.7964e-04, 'epoch': 2.12, 'throughput': 1341.47}

07/23/2024 20:01:11 - INFO - llamafactory.extras.callbacks - {'loss': 1.4379, 'learning_rate': 1.7872e-04, 'epoch': 2.16, 'throughput': 1341.38}

07/23/2024 20:01:24 - INFO - llamafactory.extras.callbacks - {'loss': 1.3952, 'learning_rate': 1.7778e-04, 'epoch': 2.21, 'throughput': 1341.18}

07/23/2024 20:01:36 - INFO - llamafactory.extras.callbacks - {'loss': 1.4264, 'learning_rate': 1.7682e-04, 'epoch': 2.26, 'throughput': 1341.34}

07/23/2024 20:01:47 - INFO - llamafactory.extras.callbacks - {'loss': 1.3380, 'learning_rate': 1.7584e-04, 'epoch': 2.31, 'throughput': 1341.60}

07/23/2024 20:01:59 - INFO - llamafactory.extras.callbacks - {'loss': 1.4218, 'learning_rate': 1.7485e-04, 'epoch': 2.36, 'throughput': 1341.89}

07/23/2024 20:02:11 - INFO - llamafactory.extras.callbacks - {'loss': 1.4123, 'learning_rate': 1.7384e-04, 'epoch': 2.40, 'throughput': 1342.00}

07/23/2024 20:02:22 - INFO - llamafactory.extras.callbacks - {'loss': 1.4202, 'learning_rate': 1.7281e-04, 'epoch': 2.45, 'throughput': 1342.31}

07/23/2024 20:02:33 - INFO - llamafactory.extras.callbacks - {'loss': 1.5023, 'learning_rate': 1.7177e-04, 'epoch': 2.50, 'throughput': 1343.00}

07/23/2024 20:02:45 - INFO - llamafactory.extras.callbacks - {'loss': 1.4174, 'learning_rate': 1.7071e-04, 'epoch': 2.55, 'throughput': 1343.23}

07/23/2024 20:02:58 - INFO - llamafactory.extras.callbacks - {'loss': 1.4507, 'learning_rate': 1.6963e-04, 'epoch': 2.60, 'throughput': 1342.71}

07/23/2024 20:03:09 - INFO - llamafactory.extras.callbacks - {'loss': 1.5488, 'learning_rate': 1.6854e-04, 'epoch': 2.64, 'throughput': 1343.19}

07/23/2024 20:03:21 - INFO - llamafactory.extras.callbacks - {'loss': 1.4343, 'learning_rate': 1.6744e-04, 'epoch': 2.69, 'throughput': 1343.63}

07/23/2024 20:03:33 - INFO - llamafactory.extras.callbacks - {'loss': 1.4653, 'learning_rate': 1.6631e-04, 'epoch': 2.74, 'throughput': 1343.54}

07/23/2024 20:03:44 - INFO - llamafactory.extras.callbacks - {'loss': 1.3222, 'learning_rate': 1.6517e-04, 'epoch': 2.79, 'throughput': 1343.78}

07/23/2024 20:03:56 - INFO - llamafactory.extras.callbacks - {'loss': 1.4398, 'learning_rate': 1.6402e-04, 'epoch': 2.84, 'throughput': 1343.86}

07/23/2024 20:04:08 - INFO - llamafactory.extras.callbacks - {'loss': 1.4203, 'learning_rate': 1.6285e-04, 'epoch': 2.88, 'throughput': 1343.83}

07/23/2024 20:04:08 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_act_content1/checkpoint-300

07/23/2024 20:04:08 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-300/tokenizer_config.json

07/23/2024 20:04:08 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-300/special_tokens_map.json

07/23/2024 20:04:22 - INFO - llamafactory.extras.callbacks - {'loss': 1.4615, 'learning_rate': 1.6167e-04, 'epoch': 2.93, 'throughput': 1340.67}

07/23/2024 20:04:33 - INFO - llamafactory.extras.callbacks - {'loss': 1.4389, 'learning_rate': 1.6048e-04, 'epoch': 2.98, 'throughput': 1340.65}

07/23/2024 20:04:45 - INFO - llamafactory.extras.callbacks - {'loss': 1.1615, 'learning_rate': 1.5927e-04, 'epoch': 3.03, 'throughput': 1340.59}

07/23/2024 20:04:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.9940, 'learning_rate': 1.5804e-04, 'epoch': 3.08, 'throughput': 1340.41}

07/23/2024 20:05:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.9817, 'learning_rate': 1.5681e-04, 'epoch': 3.12, 'throughput': 1340.45}

07/23/2024 20:05:20 - INFO - llamafactory.extras.callbacks - {'loss': 1.0470, 'learning_rate': 1.5556e-04, 'epoch': 3.17, 'throughput': 1340.52}

07/23/2024 20:05:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.9790, 'learning_rate': 1.5429e-04, 'epoch': 3.22, 'throughput': 1340.50}

07/23/2024 20:05:44 - INFO - llamafactory.extras.callbacks - {'loss': 1.0366, 'learning_rate': 1.5302e-04, 'epoch': 3.27, 'throughput': 1340.64}

07/23/2024 20:05:56 - INFO - llamafactory.extras.callbacks - {'loss': 1.0470, 'learning_rate': 1.5173e-04, 'epoch': 3.32, 'throughput': 1340.71}

07/23/2024 20:06:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.9639, 'learning_rate': 1.5044e-04, 'epoch': 3.37, 'throughput': 1340.79}

07/23/2024 20:06:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.9721, 'learning_rate': 1.4913e-04, 'epoch': 3.41, 'throughput': 1340.84}

07/23/2024 20:06:31 - INFO - llamafactory.extras.callbacks - {'loss': 1.0461, 'learning_rate': 1.4780e-04, 'epoch': 3.46, 'throughput': 1341.17}

07/23/2024 20:06:42 - INFO - llamafactory.extras.callbacks - {'loss': 1.0054, 'learning_rate': 1.4647e-04, 'epoch': 3.51, 'throughput': 1341.49}

07/23/2024 20:06:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.9900, 'learning_rate': 1.4513e-04, 'epoch': 3.56, 'throughput': 1341.90}

07/23/2024 20:07:06 - INFO - llamafactory.extras.callbacks - {'loss': 1.0248, 'learning_rate': 1.4378e-04, 'epoch': 3.61, 'throughput': 1341.85}

07/23/2024 20:07:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.9898, 'learning_rate': 1.4241e-04, 'epoch': 3.65, 'throughput': 1341.88}

07/23/2024 20:07:30 - INFO - llamafactory.extras.callbacks - {'loss': 1.0388, 'learning_rate': 1.4104e-04, 'epoch': 3.70, 'throughput': 1341.86}

07/23/2024 20:07:42 - INFO - llamafactory.extras.callbacks - {'loss': 1.0301, 'learning_rate': 1.3966e-04, 'epoch': 3.75, 'throughput': 1342.28}

07/23/2024 20:07:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.9978, 'learning_rate': 1.3827e-04, 'epoch': 3.80, 'throughput': 1342.46}

07/23/2024 20:08:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.9731, 'learning_rate': 1.3687e-04, 'epoch': 3.85, 'throughput': 1342.59}

07/23/2024 20:08:05 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_act_content1/checkpoint-400

07/23/2024 20:08:06 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-400/tokenizer_config.json

07/23/2024 20:08:06 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-400/special_tokens_map.json

07/23/2024 20:08:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.9112, 'learning_rate': 1.3546e-04, 'epoch': 3.89, 'throughput': 1340.34}

07/23/2024 20:08:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.9899, 'learning_rate': 1.3404e-04, 'epoch': 3.94, 'throughput': 1340.17}

07/23/2024 20:08:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.9221, 'learning_rate': 1.3262e-04, 'epoch': 3.99, 'throughput': 1340.20}

07/23/2024 20:08:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.9147, 'learning_rate': 1.3119e-04, 'epoch': 4.04, 'throughput': 1340.34}

07/23/2024 20:09:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.8754, 'learning_rate': 1.2975e-04, 'epoch': 4.09, 'throughput': 1340.50}

07/23/2024 20:09:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.8131, 'learning_rate': 1.2830e-04, 'epoch': 4.13, 'throughput': 1340.53}

07/23/2024 20:09:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.8198, 'learning_rate': 1.2685e-04, 'epoch': 4.18, 'throughput': 1340.38}

07/23/2024 20:09:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.8009, 'learning_rate': 1.2540e-04, 'epoch': 4.23, 'throughput': 1340.45}

07/23/2024 20:09:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.8652, 'learning_rate': 1.2393e-04, 'epoch': 4.28, 'throughput': 1340.56}

07/23/2024 20:10:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.8983, 'learning_rate': 1.2246e-04, 'epoch': 4.33, 'throughput': 1340.54}

07/23/2024 20:10:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.8778, 'learning_rate': 1.2099e-04, 'epoch': 4.38, 'throughput': 1340.63}

07/23/2024 20:10:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.9435, 'learning_rate': 1.1951e-04, 'epoch': 4.42, 'throughput': 1340.65}

07/23/2024 20:10:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.8377, 'learning_rate': 1.1803e-04, 'epoch': 4.47, 'throughput': 1340.87}

07/23/2024 20:10:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.8489, 'learning_rate': 1.1654e-04, 'epoch': 4.52, 'throughput': 1341.05}

07/23/2024 20:11:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.7999, 'learning_rate': 1.1505e-04, 'epoch': 4.57, 'throughput': 1341.25}

07/23/2024 20:11:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.7982, 'learning_rate': 1.1355e-04, 'epoch': 4.62, 'throughput': 1341.38}

07/23/2024 20:11:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.8691, 'learning_rate': 1.1205e-04, 'epoch': 4.66, 'throughput': 1341.35}

07/23/2024 20:11:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.9897, 'learning_rate': 1.1055e-04, 'epoch': 4.71, 'throughput': 1341.69}

07/23/2024 20:11:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.9016, 'learning_rate': 1.0905e-04, 'epoch': 4.76, 'throughput': 1341.66}

07/23/2024 20:12:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.8167, 'learning_rate': 1.0754e-04, 'epoch': 4.81, 'throughput': 1341.83}

07/23/2024 20:12:03 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_act_content1/checkpoint-500

07/23/2024 20:12:04 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-500/tokenizer_config.json

07/23/2024 20:12:04 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-500/special_tokens_map.json

07/23/2024 20:12:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.8919, 'learning_rate': 1.0604e-04, 'epoch': 4.86, 'throughput': 1340.03}

07/23/2024 20:12:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.8426, 'learning_rate': 1.0453e-04, 'epoch': 4.90, 'throughput': 1340.17}

07/23/2024 20:12:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.8292, 'learning_rate': 1.0302e-04, 'epoch': 4.95, 'throughput': 1340.48}

07/23/2024 20:12:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.8397, 'learning_rate': 1.0151e-04, 'epoch': 5.00, 'throughput': 1340.47}

07/23/2024 20:13:03 - INFO - llamafactory.extras.callbacks - {'loss': 0.7840, 'learning_rate': 1.0000e-04, 'epoch': 5.05, 'throughput': 1340.49}

07/23/2024 20:13:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.7444, 'learning_rate': 9.8490e-05, 'epoch': 5.10, 'throughput': 1340.50}

07/23/2024 20:13:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.8872, 'learning_rate': 9.6980e-05, 'epoch': 5.14, 'throughput': 1340.37}

07/23/2024 20:13:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.8455, 'learning_rate': 9.5470e-05, 'epoch': 5.19, 'throughput': 1340.66}

07/23/2024 20:13:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.7973, 'learning_rate': 9.3962e-05, 'epoch': 5.24, 'throughput': 1340.88}

07/23/2024 20:14:02 - INFO - llamafactory.extras.callbacks - {'loss': 0.8499, 'learning_rate': 9.2455e-05, 'epoch': 5.29, 'throughput': 1340.90}

07/23/2024 20:14:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.7522, 'learning_rate': 9.0950e-05, 'epoch': 5.34, 'throughput': 1340.94}

07/23/2024 20:14:26 - INFO - llamafactory.extras.callbacks - {'loss': 0.8445, 'learning_rate': 8.9447e-05, 'epoch': 5.38, 'throughput': 1341.03}

07/23/2024 20:14:38 - INFO - llamafactory.extras.callbacks - {'loss': 0.9061, 'learning_rate': 8.7946e-05, 'epoch': 5.43, 'throughput': 1341.00}

07/23/2024 20:14:50 - INFO - llamafactory.extras.callbacks - {'loss': 0.7859, 'learning_rate': 8.6448e-05, 'epoch': 5.48, 'throughput': 1341.19}

07/23/2024 20:15:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.8670, 'learning_rate': 8.4954e-05, 'epoch': 5.53, 'throughput': 1341.26}

07/23/2024 20:15:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.8410, 'learning_rate': 8.3462e-05, 'epoch': 5.58, 'throughput': 1341.30}

07/23/2024 20:15:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.8759, 'learning_rate': 8.1974e-05, 'epoch': 5.62, 'throughput': 1341.34}

07/23/2024 20:15:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.7427, 'learning_rate': 8.0491e-05, 'epoch': 5.67, 'throughput': 1341.25}

07/23/2024 20:15:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.8661, 'learning_rate': 7.9012e-05, 'epoch': 5.72, 'throughput': 1341.24}

07/23/2024 20:16:00 - INFO - llamafactory.extras.callbacks - {'loss': 0.8267, 'learning_rate': 7.7538e-05, 'epoch': 5.77, 'throughput': 1341.39}

07/23/2024 20:16:00 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_act_content1/checkpoint-600

07/23/2024 20:16:01 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-600/tokenizer_config.json

07/23/2024 20:16:01 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-600/special_tokens_map.json

07/23/2024 20:16:14 - INFO - llamafactory.extras.callbacks - {'loss': 0.8063, 'learning_rate': 7.6068e-05, 'epoch': 5.82, 'throughput': 1339.97}

07/23/2024 20:16:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.7442, 'learning_rate': 7.4605e-05, 'epoch': 5.87, 'throughput': 1340.20}

07/23/2024 20:16:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.7929, 'learning_rate': 7.3147e-05, 'epoch': 5.91, 'throughput': 1340.25}

07/23/2024 20:16:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.8310, 'learning_rate': 7.1695e-05, 'epoch': 5.96, 'throughput': 1340.07}

07/23/2024 20:17:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.7548, 'learning_rate': 7.0250e-05, 'epoch': 6.01, 'throughput': 1340.19}

07/23/2024 20:17:13 - INFO - llamafactory.extras.callbacks - {'loss': 0.7878, 'learning_rate': 6.8811e-05, 'epoch': 6.06, 'throughput': 1340.48}

07/23/2024 20:17:25 - INFO - llamafactory.extras.callbacks - {'loss': 0.7435, 'learning_rate': 6.7380e-05, 'epoch': 6.11, 'throughput': 1340.48}

07/23/2024 20:17:37 - INFO - llamafactory.extras.callbacks - {'loss': 0.8156, 'learning_rate': 6.5956e-05, 'epoch': 6.15, 'throughput': 1340.51}

07/23/2024 20:17:49 - INFO - llamafactory.extras.callbacks - {'loss': 0.7670, 'learning_rate': 6.4540e-05, 'epoch': 6.20, 'throughput': 1340.35}

07/23/2024 20:18:01 - INFO - llamafactory.extras.callbacks - {'loss': 0.7959, 'learning_rate': 6.3131e-05, 'epoch': 6.25, 'throughput': 1340.53}

07/23/2024 20:18:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.8080, 'learning_rate': 6.1732e-05, 'epoch': 6.30, 'throughput': 1340.66}

07/23/2024 20:18:24 - INFO - llamafactory.extras.callbacks - {'loss': 0.7473, 'learning_rate': 6.0341e-05, 'epoch': 6.35, 'throughput': 1340.76}

07/23/2024 20:18:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.7845, 'learning_rate': 5.8959e-05, 'epoch': 6.39, 'throughput': 1340.91}

07/23/2024 20:18:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.7581, 'learning_rate': 5.7586e-05, 'epoch': 6.44, 'throughput': 1340.85}

07/23/2024 20:18:59 - INFO - llamafactory.extras.callbacks - {'loss': 0.7656, 'learning_rate': 5.6223e-05, 'epoch': 6.49, 'throughput': 1341.07}

07/23/2024 20:19:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.8021, 'learning_rate': 5.4870e-05, 'epoch': 6.54, 'throughput': 1341.02}

07/23/2024 20:19:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.7533, 'learning_rate': 5.3528e-05, 'epoch': 6.59, 'throughput': 1341.14}

07/23/2024 20:19:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.7708, 'learning_rate': 5.2196e-05, 'epoch': 6.63, 'throughput': 1341.17}

07/23/2024 20:19:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.8698, 'learning_rate': 5.0875e-05, 'epoch': 6.68, 'throughput': 1341.12}

07/23/2024 20:19:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.8328, 'learning_rate': 4.9565e-05, 'epoch': 6.73, 'throughput': 1341.14}

07/23/2024 20:19:58 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_act_content1/checkpoint-700

07/23/2024 20:19:59 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-700/tokenizer_config.json

07/23/2024 20:19:59 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-700/special_tokens_map.json

07/23/2024 20:20:12 - INFO - llamafactory.extras.callbacks - {'loss': 0.7665, 'learning_rate': 4.8266e-05, 'epoch': 6.78, 'throughput': 1339.94}

07/23/2024 20:20:23 - INFO - llamafactory.extras.callbacks - {'loss': 0.7451, 'learning_rate': 4.6980e-05, 'epoch': 6.83, 'throughput': 1339.91}

07/23/2024 20:20:35 - INFO - llamafactory.extras.callbacks - {'loss': 0.7950, 'learning_rate': 4.5705e-05, 'epoch': 6.88, 'throughput': 1339.90}

07/23/2024 20:20:47 - INFO - llamafactory.extras.callbacks - {'loss': 0.7641, 'learning_rate': 4.4443e-05, 'epoch': 6.92, 'throughput': 1339.97}

07/23/2024 20:20:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.8405, 'learning_rate': 4.3194e-05, 'epoch': 6.97, 'throughput': 1340.12}

07/23/2024 20:21:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.8222, 'learning_rate': 4.1957e-05, 'epoch': 7.02, 'throughput': 1339.94}

07/23/2024 20:21:22 - INFO - llamafactory.extras.callbacks - {'loss': 0.7656, 'learning_rate': 4.0734e-05, 'epoch': 7.07, 'throughput': 1339.85}

07/23/2024 20:21:34 - INFO - llamafactory.extras.callbacks - {'loss': 0.7195, 'learning_rate': 3.9524e-05, 'epoch': 7.12, 'throughput': 1339.96}

07/23/2024 20:21:46 - INFO - llamafactory.extras.callbacks - {'loss': 0.7456, 'learning_rate': 3.8328e-05, 'epoch': 7.16, 'throughput': 1339.95}

07/23/2024 20:21:58 - INFO - llamafactory.extras.callbacks - {'loss': 0.7792, 'learning_rate': 3.7146e-05, 'epoch': 7.21, 'throughput': 1340.16}

07/23/2024 20:22:09 - INFO - llamafactory.extras.callbacks - {'loss': 0.7662, 'learning_rate': 3.5979e-05, 'epoch': 7.26, 'throughput': 1340.24}

07/23/2024 20:22:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.7448, 'learning_rate': 3.4826e-05, 'epoch': 7.31, 'throughput': 1340.30}

07/23/2024 20:22:33 - INFO - llamafactory.extras.callbacks - {'loss': 0.7514, 'learning_rate': 3.3688e-05, 'epoch': 7.36, 'throughput': 1340.23}

07/23/2024 20:22:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.7507, 'learning_rate': 3.2565e-05, 'epoch': 7.40, 'throughput': 1340.25}

07/23/2024 20:22:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.8242, 'learning_rate': 3.1457e-05, 'epoch': 7.45, 'throughput': 1340.18}

07/23/2024 20:23:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.7396, 'learning_rate': 3.0365e-05, 'epoch': 7.50, 'throughput': 1340.21}

07/23/2024 20:23:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.7978, 'learning_rate': 2.9289e-05, 'epoch': 7.55, 'throughput': 1340.28}

07/23/2024 20:23:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.7739, 'learning_rate': 2.8229e-05, 'epoch': 7.60, 'throughput': 1340.33}

07/23/2024 20:23:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.7836, 'learning_rate': 2.7186e-05, 'epoch': 7.64, 'throughput': 1340.28}

07/23/2024 20:23:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.8154, 'learning_rate': 2.6159e-05, 'epoch': 7.69, 'throughput': 1340.31}

07/23/2024 20:23:56 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_act_content1/checkpoint-800

07/23/2024 20:23:56 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-800/tokenizer_config.json

07/23/2024 20:23:57 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-800/special_tokens_map.json

07/23/2024 20:24:10 - INFO - llamafactory.extras.callbacks - {'loss': 0.7778, 'learning_rate': 2.5149e-05, 'epoch': 7.74, 'throughput': 1339.12}

07/23/2024 20:24:21 - INFO - llamafactory.extras.callbacks - {'loss': 0.7378, 'learning_rate': 2.4156e-05, 'epoch': 7.79, 'throughput': 1339.25}

07/23/2024 20:24:33 - INFO - llamafactory.extras.callbacks - {'loss': 0.7947, 'learning_rate': 2.3180e-05, 'epoch': 7.84, 'throughput': 1339.35}

07/23/2024 20:24:45 - INFO - llamafactory.extras.callbacks - {'loss': 0.7693, 'learning_rate': 2.2222e-05, 'epoch': 7.88, 'throughput': 1339.41}

07/23/2024 20:24:57 - INFO - llamafactory.extras.callbacks - {'loss': 0.7894, 'learning_rate': 2.1282e-05, 'epoch': 7.93, 'throughput': 1339.57}

07/23/2024 20:25:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.7937, 'learning_rate': 2.0359e-05, 'epoch': 7.98, 'throughput': 1339.67}

07/23/2024 20:25:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.8277, 'learning_rate': 1.9455e-05, 'epoch': 8.03, 'throughput': 1339.64}

07/23/2024 20:25:32 - INFO - llamafactory.extras.callbacks - {'loss': 0.7463, 'learning_rate': 1.8569e-05, 'epoch': 8.08, 'throughput': 1339.69}

07/23/2024 20:25:44 - INFO - llamafactory.extras.callbacks - {'loss': 0.8220, 'learning_rate': 1.7702e-05, 'epoch': 8.12, 'throughput': 1339.64}

07/23/2024 20:25:56 - INFO - llamafactory.extras.callbacks - {'loss': 0.7390, 'learning_rate': 1.6853e-05, 'epoch': 8.17, 'throughput': 1339.55}

07/23/2024 20:26:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.7163, 'learning_rate': 1.6023e-05, 'epoch': 8.22, 'throughput': 1339.69}

07/23/2024 20:26:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.6851, 'learning_rate': 1.5213e-05, 'epoch': 8.27, 'throughput': 1339.85}

07/23/2024 20:26:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.7687, 'learning_rate': 1.4422e-05, 'epoch': 8.32, 'throughput': 1339.91}

07/23/2024 20:26:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.7442, 'learning_rate': 1.3650e-05, 'epoch': 8.37, 'throughput': 1339.92}

07/23/2024 20:26:55 - INFO - llamafactory.extras.callbacks - {'loss': 0.8365, 'learning_rate': 1.2898e-05, 'epoch': 8.41, 'throughput': 1340.03}

07/23/2024 20:27:07 - INFO - llamafactory.extras.callbacks - {'loss': 0.7603, 'learning_rate': 1.2166e-05, 'epoch': 8.46, 'throughput': 1340.12}

07/23/2024 20:27:19 - INFO - llamafactory.extras.callbacks - {'loss': 0.7010, 'learning_rate': 1.1454e-05, 'epoch': 8.51, 'throughput': 1340.13}

07/23/2024 20:27:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.7924, 'learning_rate': 1.0763e-05, 'epoch': 8.56, 'throughput': 1340.14}

07/23/2024 20:27:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.8054, 'learning_rate': 1.0091e-05, 'epoch': 8.61, 'throughput': 1340.28}

07/23/2024 20:27:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.7235, 'learning_rate': 9.4403e-06, 'epoch': 8.65, 'throughput': 1340.37}

07/23/2024 20:27:54 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_act_content1/checkpoint-900

07/23/2024 20:27:55 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-900/tokenizer_config.json

07/23/2024 20:27:55 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-900/special_tokens_map.json

07/23/2024 20:28:08 - INFO - llamafactory.extras.callbacks - {'loss': 0.7803, 'learning_rate': 8.8100e-06, 'epoch': 8.70, 'throughput': 1339.31}

07/23/2024 20:28:20 - INFO - llamafactory.extras.callbacks - {'loss': 0.7539, 'learning_rate': 8.2006e-06, 'epoch': 8.75, 'throughput': 1339.34}

07/23/2024 20:28:31 - INFO - llamafactory.extras.callbacks - {'loss': 0.7518, 'learning_rate': 7.6120e-06, 'epoch': 8.80, 'throughput': 1339.41}

07/23/2024 20:28:43 - INFO - llamafactory.extras.callbacks - {'loss': 0.7945, 'learning_rate': 7.0446e-06, 'epoch': 8.85, 'throughput': 1339.53}

07/23/2024 20:28:54 - INFO - llamafactory.extras.callbacks - {'loss': 0.8341, 'learning_rate': 6.4984e-06, 'epoch': 8.89, 'throughput': 1339.59}

07/23/2024 20:29:06 - INFO - llamafactory.extras.callbacks - {'loss': 0.7938, 'learning_rate': 5.9735e-06, 'epoch': 8.94, 'throughput': 1339.62}

07/23/2024 20:29:18 - INFO - llamafactory.extras.callbacks - {'loss': 0.7768, 'learning_rate': 5.4700e-06, 'epoch': 8.99, 'throughput': 1339.67}

07/23/2024 20:29:30 - INFO - llamafactory.extras.callbacks - {'loss': 0.7522, 'learning_rate': 4.9881e-06, 'epoch': 9.04, 'throughput': 1339.68}

07/23/2024 20:29:42 - INFO - llamafactory.extras.callbacks - {'loss': 0.7252, 'learning_rate': 4.5279e-06, 'epoch': 9.09, 'throughput': 1339.79}

07/23/2024 20:29:53 - INFO - llamafactory.extras.callbacks - {'loss': 0.7897, 'learning_rate': 4.0895e-06, 'epoch': 9.13, 'throughput': 1339.87}

07/23/2024 20:30:05 - INFO - llamafactory.extras.callbacks - {'loss': 0.7414, 'learning_rate': 3.6729e-06, 'epoch': 9.18, 'throughput': 1339.95}

07/23/2024 20:30:17 - INFO - llamafactory.extras.callbacks - {'loss': 0.7578, 'learning_rate': 3.2783e-06, 'epoch': 9.23, 'throughput': 1339.96}

07/23/2024 20:30:29 - INFO - llamafactory.extras.callbacks - {'loss': 0.7626, 'learning_rate': 2.9058e-06, 'epoch': 9.28, 'throughput': 1340.10}

07/23/2024 20:30:41 - INFO - llamafactory.extras.callbacks - {'loss': 0.7965, 'learning_rate': 2.5554e-06, 'epoch': 9.33, 'throughput': 1340.08}

07/23/2024 20:30:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.7608, 'learning_rate': 2.2273e-06, 'epoch': 9.38, 'throughput': 1340.10}

07/23/2024 20:31:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.7683, 'learning_rate': 1.9215e-06, 'epoch': 9.42, 'throughput': 1340.18}

07/23/2024 20:31:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.7513, 'learning_rate': 1.6380e-06, 'epoch': 9.47, 'throughput': 1340.20}

07/23/2024 20:31:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.8062, 'learning_rate': 1.3770e-06, 'epoch': 9.52, 'throughput': 1340.20}

07/23/2024 20:31:39 - INFO - llamafactory.extras.callbacks - {'loss': 0.8195, 'learning_rate': 1.1385e-06, 'epoch': 9.57, 'throughput': 1340.35}

07/23/2024 20:31:51 - INFO - llamafactory.extras.callbacks - {'loss': 0.7567, 'learning_rate': 9.2248e-07, 'epoch': 9.62, 'throughput': 1340.38}

07/23/2024 20:31:51 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_act_content1/checkpoint-1000

07/23/2024 20:31:51 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-1000/tokenizer_config.json

07/23/2024 20:31:51 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-1000/special_tokens_map.json

07/23/2024 20:32:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.7812, 'learning_rate': 7.2911e-07, 'epoch': 9.66, 'throughput': 1339.53}

07/23/2024 20:32:16 - INFO - llamafactory.extras.callbacks - {'loss': 0.8083, 'learning_rate': 5.5839e-07, 'epoch': 9.71, 'throughput': 1339.65}

07/23/2024 20:32:28 - INFO - llamafactory.extras.callbacks - {'loss': 0.7446, 'learning_rate': 4.1034e-07, 'epoch': 9.76, 'throughput': 1339.69}

07/23/2024 20:32:40 - INFO - llamafactory.extras.callbacks - {'loss': 0.7800, 'learning_rate': 2.8502e-07, 'epoch': 9.81, 'throughput': 1339.64}

07/23/2024 20:32:52 - INFO - llamafactory.extras.callbacks - {'loss': 0.7341, 'learning_rate': 1.8244e-07, 'epoch': 9.86, 'throughput': 1339.70}

07/23/2024 20:33:04 - INFO - llamafactory.extras.callbacks - {'loss': 0.7459, 'learning_rate': 1.0264e-07, 'epoch': 9.90, 'throughput': 1339.68}

07/23/2024 20:33:15 - INFO - llamafactory.extras.callbacks - {'loss': 0.8130, 'learning_rate': 4.5622e-08, 'epoch': 9.95, 'throughput': 1339.80}

07/23/2024 20:33:27 - INFO - llamafactory.extras.callbacks - {'loss': 0.7611, 'learning_rate': 1.1406e-08, 'epoch': 10.00, 'throughput': 1339.73}

07/23/2024 20:33:27 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_act_content1/checkpoint-1040

07/23/2024 20:33:28 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-1040/tokenizer_config.json

07/23/2024 20:33:28 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_act_content1/checkpoint-1040/special_tokens_map.json

07/23/2024 20:33:29 - INFO - transformers.trainer - 

Training completed. Do not forget to share your model on huggingface.co/models =)



07/23/2024 20:33:29 - INFO - transformers.trainer - Saving model checkpoint to saves/LLaMA3-8B/lora/train_act_content1

07/23/2024 20:33:29 - INFO - transformers.tokenization_utils_base - tokenizer config file saved in saves/LLaMA3-8B/lora/train_act_content1/tokenizer_config.json

07/23/2024 20:33:29 - INFO - transformers.tokenization_utils_base - Special tokens file saved in saves/LLaMA3-8B/lora/train_act_content1/special_tokens_map.json

07/23/2024 20:33:29 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.

07/23/2024 20:33:29 - INFO - transformers.modelcard - Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}

