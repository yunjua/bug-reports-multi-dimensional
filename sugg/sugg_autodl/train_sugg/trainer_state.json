{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.977324263038549,
  "eval_steps": 500,
  "global_step": 550,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.09070294784580499,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 7.1465,
      "num_input_tokens_seen": 14400,
      "step": 5
    },
    {
      "epoch": 0.18140589569160998,
      "grad_norm": 10.630600929260254,
      "learning_rate": 0.0001999738997023281,
      "loss": 7.5043,
      "num_input_tokens_seen": 28576,
      "step": 10
    },
    {
      "epoch": 0.272108843537415,
      "grad_norm": 7.220844268798828,
      "learning_rate": 0.00019986789059320615,
      "loss": 4.9654,
      "num_input_tokens_seen": 43120,
      "step": 15
    },
    {
      "epoch": 0.36281179138321995,
      "grad_norm": 7.17600679397583,
      "learning_rate": 0.00019968042780004917,
      "loss": 4.1498,
      "num_input_tokens_seen": 57664,
      "step": 20
    },
    {
      "epoch": 0.45351473922902497,
      "grad_norm": 4.623558521270752,
      "learning_rate": 0.00019941166422020014,
      "loss": 3.9033,
      "num_input_tokens_seen": 71680,
      "step": 25
    },
    {
      "epoch": 0.54421768707483,
      "grad_norm": 5.158267498016357,
      "learning_rate": 0.00019906181906108984,
      "loss": 3.8342,
      "num_input_tokens_seen": 86096,
      "step": 30
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 5.77419376373291,
      "learning_rate": 0.00019863117766144806,
      "loss": 4.0354,
      "num_input_tokens_seen": 100272,
      "step": 35
    },
    {
      "epoch": 0.7256235827664399,
      "grad_norm": 4.27996826171875,
      "learning_rate": 0.00019812009125857728,
      "loss": 3.5949,
      "num_input_tokens_seen": 114640,
      "step": 40
    },
    {
      "epoch": 0.8163265306122449,
      "grad_norm": 4.899968147277832,
      "learning_rate": 0.0001975289767018786,
      "loss": 3.4396,
      "num_input_tokens_seen": 128880,
      "step": 45
    },
    {
      "epoch": 0.9070294784580499,
      "grad_norm": 4.737531661987305,
      "learning_rate": 0.0001968583161128631,
      "loss": 3.8235,
      "num_input_tokens_seen": 142448,
      "step": 50
    },
    {
      "epoch": 0.9977324263038548,
      "grad_norm": 6.063131809234619,
      "learning_rate": 0.00019610865649192697,
      "loss": 3.5458,
      "num_input_tokens_seen": 156640,
      "step": 55
    },
    {
      "epoch": 1.08843537414966,
      "grad_norm": 9.081221580505371,
      "learning_rate": 0.0001952806092722098,
      "loss": 3.2139,
      "num_input_tokens_seen": 171008,
      "step": 60
    },
    {
      "epoch": 1.179138321995465,
      "grad_norm": 4.139898777008057,
      "learning_rate": 0.0001943748498209012,
      "loss": 3.2016,
      "num_input_tokens_seen": 185024,
      "step": 65
    },
    {
      "epoch": 1.2698412698412698,
      "grad_norm": 3.9867465496063232,
      "learning_rate": 0.00019339211688840157,
      "loss": 2.9983,
      "num_input_tokens_seen": 199264,
      "step": 70
    },
    {
      "epoch": 1.3605442176870748,
      "grad_norm": 4.396787643432617,
      "learning_rate": 0.0001923332120057866,
      "loss": 3.0781,
      "num_input_tokens_seen": 213296,
      "step": 75
    },
    {
      "epoch": 1.4512471655328798,
      "grad_norm": 5.038854122161865,
      "learning_rate": 0.000191198998831067,
      "loss": 2.9597,
      "num_input_tokens_seen": 227392,
      "step": 80
    },
    {
      "epoch": 1.5419501133786848,
      "grad_norm": 3.666499614715576,
      "learning_rate": 0.0001899904024447769,
      "loss": 2.8922,
      "num_input_tokens_seen": 240992,
      "step": 85
    },
    {
      "epoch": 1.6326530612244898,
      "grad_norm": 4.529850482940674,
      "learning_rate": 0.00018870840859546456,
      "loss": 2.8785,
      "num_input_tokens_seen": 255392,
      "step": 90
    },
    {
      "epoch": 1.7233560090702946,
      "grad_norm": 5.564001560211182,
      "learning_rate": 0.00018735406289570192,
      "loss": 2.9013,
      "num_input_tokens_seen": 269552,
      "step": 95
    },
    {
      "epoch": 1.8140589569160999,
      "grad_norm": 5.481484889984131,
      "learning_rate": 0.00018592846996926793,
      "loss": 2.8602,
      "num_input_tokens_seen": 283696,
      "step": 100
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 5.227039813995361,
      "learning_rate": 0.00018443279255020152,
      "loss": 3.0984,
      "num_input_tokens_seen": 297824,
      "step": 105
    },
    {
      "epoch": 1.99546485260771,
      "grad_norm": 4.371924877166748,
      "learning_rate": 0.00018286825053445918,
      "loss": 2.9563,
      "num_input_tokens_seen": 312720,
      "step": 110
    },
    {
      "epoch": 2.0861678004535147,
      "grad_norm": 3.9992995262145996,
      "learning_rate": 0.00018123611998495007,
      "loss": 2.2437,
      "num_input_tokens_seen": 326992,
      "step": 115
    },
    {
      "epoch": 2.17687074829932,
      "grad_norm": 7.326725959777832,
      "learning_rate": 0.0001795377320907611,
      "loss": 2.2673,
      "num_input_tokens_seen": 340992,
      "step": 120
    },
    {
      "epoch": 2.2675736961451247,
      "grad_norm": 4.517050743103027,
      "learning_rate": 0.0001777744720814198,
      "loss": 2.0066,
      "num_input_tokens_seen": 355152,
      "step": 125
    },
    {
      "epoch": 2.35827664399093,
      "grad_norm": 5.994265079498291,
      "learning_rate": 0.00017594777809708126,
      "loss": 2.1338,
      "num_input_tokens_seen": 369072,
      "step": 130
    },
    {
      "epoch": 2.4489795918367347,
      "grad_norm": 6.31696891784668,
      "learning_rate": 0.0001740591400155606,
      "loss": 1.9637,
      "num_input_tokens_seen": 383328,
      "step": 135
    },
    {
      "epoch": 2.5396825396825395,
      "grad_norm": 5.447580814361572,
      "learning_rate": 0.00017211009823716694,
      "loss": 2.1357,
      "num_input_tokens_seen": 397632,
      "step": 140
    },
    {
      "epoch": 2.630385487528345,
      "grad_norm": 6.874329566955566,
      "learning_rate": 0.0001701022424283311,
      "loss": 2.189,
      "num_input_tokens_seen": 411648,
      "step": 145
    },
    {
      "epoch": 2.7210884353741496,
      "grad_norm": 5.537710666656494,
      "learning_rate": 0.00016803721022505067,
      "loss": 2.0305,
      "num_input_tokens_seen": 425872,
      "step": 150
    },
    {
      "epoch": 2.811791383219955,
      "grad_norm": 5.767446041107178,
      "learning_rate": 0.0001659166858972107,
      "loss": 2.01,
      "num_input_tokens_seen": 439792,
      "step": 155
    },
    {
      "epoch": 2.9024943310657596,
      "grad_norm": 6.2734832763671875,
      "learning_rate": 0.000163742398974869,
      "loss": 2.0976,
      "num_input_tokens_seen": 454320,
      "step": 160
    },
    {
      "epoch": 2.9931972789115644,
      "grad_norm": 7.538093566894531,
      "learning_rate": 0.00016151612283762652,
      "loss": 2.2041,
      "num_input_tokens_seen": 468496,
      "step": 165
    },
    {
      "epoch": 3.0839002267573696,
      "grad_norm": 5.360706806182861,
      "learning_rate": 0.00015923967326823368,
      "loss": 1.5774,
      "num_input_tokens_seen": 482576,
      "step": 170
    },
    {
      "epoch": 3.1746031746031744,
      "grad_norm": 12.459641456604004,
      "learning_rate": 0.00015691490697161182,
      "loss": 1.5774,
      "num_input_tokens_seen": 496832,
      "step": 175
    },
    {
      "epoch": 3.2653061224489797,
      "grad_norm": 5.677186965942383,
      "learning_rate": 0.00015454372006049803,
      "loss": 1.5205,
      "num_input_tokens_seen": 510992,
      "step": 180
    },
    {
      "epoch": 3.3560090702947845,
      "grad_norm": 4.850758075714111,
      "learning_rate": 0.0001521280465089484,
      "loss": 1.4796,
      "num_input_tokens_seen": 525216,
      "step": 185
    },
    {
      "epoch": 3.4467120181405897,
      "grad_norm": 5.789063930511475,
      "learning_rate": 0.00014966985657496114,
      "loss": 1.4751,
      "num_input_tokens_seen": 539648,
      "step": 190
    },
    {
      "epoch": 3.5374149659863945,
      "grad_norm": 6.409037113189697,
      "learning_rate": 0.00014717115519350567,
      "loss": 1.4612,
      "num_input_tokens_seen": 553808,
      "step": 195
    },
    {
      "epoch": 3.6281179138321997,
      "grad_norm": 5.017320156097412,
      "learning_rate": 0.0001446339803412692,
      "loss": 1.4342,
      "num_input_tokens_seen": 567904,
      "step": 200
    },
    {
      "epoch": 3.7188208616780045,
      "grad_norm": 5.705946445465088,
      "learning_rate": 0.00014206040137445348,
      "loss": 1.4051,
      "num_input_tokens_seen": 581808,
      "step": 205
    },
    {
      "epoch": 3.8095238095238093,
      "grad_norm": 5.435763359069824,
      "learning_rate": 0.00013945251734097828,
      "loss": 1.4083,
      "num_input_tokens_seen": 596048,
      "step": 210
    },
    {
      "epoch": 3.9002267573696145,
      "grad_norm": 6.648280620574951,
      "learning_rate": 0.00013681245526846783,
      "loss": 1.4684,
      "num_input_tokens_seen": 609904,
      "step": 215
    },
    {
      "epoch": 3.9909297052154193,
      "grad_norm": 4.899482727050781,
      "learning_rate": 0.00013414236842941644,
      "loss": 1.2983,
      "num_input_tokens_seen": 624256,
      "step": 220
    },
    {
      "epoch": 4.081632653061225,
      "grad_norm": 5.635927677154541,
      "learning_rate": 0.00013144443458494882,
      "loss": 1.1009,
      "num_input_tokens_seen": 638448,
      "step": 225
    },
    {
      "epoch": 4.172335600907029,
      "grad_norm": 4.545168399810791,
      "learning_rate": 0.00012872085420860665,
      "loss": 1.1704,
      "num_input_tokens_seen": 652352,
      "step": 230
    },
    {
      "epoch": 4.263038548752834,
      "grad_norm": 5.037203311920166,
      "learning_rate": 0.00012597384869161084,
      "loss": 1.1719,
      "num_input_tokens_seen": 666320,
      "step": 235
    },
    {
      "epoch": 4.35374149659864,
      "grad_norm": 2.0793843269348145,
      "learning_rate": 0.00012320565853106316,
      "loss": 1.1529,
      "num_input_tokens_seen": 680528,
      "step": 240
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 6.0266900062561035,
      "learning_rate": 0.00012041854150256433,
      "loss": 1.205,
      "num_input_tokens_seen": 694752,
      "step": 245
    },
    {
      "epoch": 4.535147392290249,
      "grad_norm": 3.113672971725464,
      "learning_rate": 0.00011761477081874015,
      "loss": 1.0947,
      "num_input_tokens_seen": 708992,
      "step": 250
    },
    {
      "epoch": 4.625850340136054,
      "grad_norm": 4.969809055328369,
      "learning_rate": 0.00011479663327517667,
      "loss": 1.2141,
      "num_input_tokens_seen": 722912,
      "step": 255
    },
    {
      "epoch": 4.71655328798186,
      "grad_norm": 2.0930793285369873,
      "learning_rate": 0.00011196642738527659,
      "loss": 1.1449,
      "num_input_tokens_seen": 737392,
      "step": 260
    },
    {
      "epoch": 4.807256235827665,
      "grad_norm": 3.1693108081817627,
      "learning_rate": 0.00010912646150555919,
      "loss": 1.1608,
      "num_input_tokens_seen": 751440,
      "step": 265
    },
    {
      "epoch": 4.8979591836734695,
      "grad_norm": 5.593299388885498,
      "learning_rate": 0.00010627905195293135,
      "loss": 1.1301,
      "num_input_tokens_seen": 765792,
      "step": 270
    },
    {
      "epoch": 4.988662131519274,
      "grad_norm": 6.650789737701416,
      "learning_rate": 0.00010342652111546635,
      "loss": 1.1257,
      "num_input_tokens_seen": 780224,
      "step": 275
    },
    {
      "epoch": 5.079365079365079,
      "grad_norm": 7.867834091186523,
      "learning_rate": 0.00010057119555823085,
      "loss": 1.1448,
      "num_input_tokens_seen": 794368,
      "step": 280
    },
    {
      "epoch": 5.170068027210885,
      "grad_norm": 2.981612205505371,
      "learning_rate": 9.771540412570504e-05,
      "loss": 1.0064,
      "num_input_tokens_seen": 808912,
      "step": 285
    },
    {
      "epoch": 5.26077097505669,
      "grad_norm": 1.583750605583191,
      "learning_rate": 9.486147604234371e-05,
      "loss": 1.024,
      "num_input_tokens_seen": 822896,
      "step": 290
    },
    {
      "epoch": 5.351473922902494,
      "grad_norm": 4.79334020614624,
      "learning_rate": 9.201173901282724e-05,
      "loss": 1.0705,
      "num_input_tokens_seen": 836960,
      "step": 295
    },
    {
      "epoch": 5.442176870748299,
      "grad_norm": 1.8876078128814697,
      "learning_rate": 8.916851732355255e-05,
      "loss": 1.0586,
      "num_input_tokens_seen": 851120,
      "step": 300
    },
    {
      "epoch": 5.532879818594104,
      "grad_norm": 5.813927173614502,
      "learning_rate": 8.633412994691144e-05,
      "loss": 1.0807,
      "num_input_tokens_seen": 865056,
      "step": 305
    },
    {
      "epoch": 5.62358276643991,
      "grad_norm": 1.9863964319229126,
      "learning_rate": 8.351088864990368e-05,
      "loss": 1.0891,
      "num_input_tokens_seen": 878912,
      "step": 310
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 3.601102352142334,
      "learning_rate": 8.070109610862668e-05,
      "loss": 0.9968,
      "num_input_tokens_seen": 893056,
      "step": 315
    },
    {
      "epoch": 5.804988662131519,
      "grad_norm": 4.725615978240967,
      "learning_rate": 7.79070440301796e-05,
      "loss": 1.0081,
      "num_input_tokens_seen": 907040,
      "step": 320
    },
    {
      "epoch": 5.895691609977324,
      "grad_norm": 5.142914295196533,
      "learning_rate": 7.513101128351454e-05,
      "loss": 1.0262,
      "num_input_tokens_seen": 921568,
      "step": 325
    },
    {
      "epoch": 5.986394557823129,
      "grad_norm": 4.642932891845703,
      "learning_rate": 7.237526204075797e-05,
      "loss": 1.0199,
      "num_input_tokens_seen": 935888,
      "step": 330
    },
    {
      "epoch": 6.0770975056689345,
      "grad_norm": 3.7066774368286133,
      "learning_rate": 6.964204393051981e-05,
      "loss": 0.9648,
      "num_input_tokens_seen": 950016,
      "step": 335
    },
    {
      "epoch": 6.167800453514739,
      "grad_norm": 1.2093629837036133,
      "learning_rate": 6.693358620469487e-05,
      "loss": 1.035,
      "num_input_tokens_seen": 964240,
      "step": 340
    },
    {
      "epoch": 6.258503401360544,
      "grad_norm": 2.0800716876983643,
      "learning_rate": 6.425209792025358e-05,
      "loss": 1.0043,
      "num_input_tokens_seen": 978224,
      "step": 345
    },
    {
      "epoch": 6.349206349206349,
      "grad_norm": 2.8759946823120117,
      "learning_rate": 6.159976613750286e-05,
      "loss": 0.9534,
      "num_input_tokens_seen": 992496,
      "step": 350
    },
    {
      "epoch": 6.4399092970521545,
      "grad_norm": 1.1151679754257202,
      "learning_rate": 5.897875413628884e-05,
      "loss": 0.9829,
      "num_input_tokens_seen": 1006736,
      "step": 355
    },
    {
      "epoch": 6.530612244897959,
      "grad_norm": 1.9035398960113525,
      "learning_rate": 5.639119965159446e-05,
      "loss": 1.0698,
      "num_input_tokens_seen": 1020816,
      "step": 360
    },
    {
      "epoch": 6.621315192743764,
      "grad_norm": 1.4604806900024414,
      "learning_rate": 5.383921312997242e-05,
      "loss": 0.9812,
      "num_input_tokens_seen": 1034992,
      "step": 365
    },
    {
      "epoch": 6.712018140589569,
      "grad_norm": 1.3253607749938965,
      "learning_rate": 5.132487600823438e-05,
      "loss": 1.0634,
      "num_input_tokens_seen": 1049488,
      "step": 370
    },
    {
      "epoch": 6.802721088435375,
      "grad_norm": 2.01686429977417,
      "learning_rate": 4.8850239015801625e-05,
      "loss": 0.932,
      "num_input_tokens_seen": 1063616,
      "step": 375
    },
    {
      "epoch": 6.893424036281179,
      "grad_norm": 0.7634481191635132,
      "learning_rate": 4.6417320502100316e-05,
      "loss": 1.0107,
      "num_input_tokens_seen": 1077536,
      "step": 380
    },
    {
      "epoch": 6.984126984126984,
      "grad_norm": 3.391744613647461,
      "learning_rate": 4.402810479036725e-05,
      "loss": 0.9947,
      "num_input_tokens_seen": 1091888,
      "step": 385
    },
    {
      "epoch": 7.074829931972789,
      "grad_norm": 0.5709068775177002,
      "learning_rate": 4.168454055920681e-05,
      "loss": 1.0348,
      "num_input_tokens_seen": 1106448,
      "step": 390
    },
    {
      "epoch": 7.165532879818594,
      "grad_norm": 0.1577218472957611,
      "learning_rate": 3.938853925322118e-05,
      "loss": 0.8845,
      "num_input_tokens_seen": 1120880,
      "step": 395
    },
    {
      "epoch": 7.2562358276643995,
      "grad_norm": 0.9977515339851379,
      "learning_rate": 3.714197352400849e-05,
      "loss": 1.0222,
      "num_input_tokens_seen": 1135024,
      "step": 400
    },
    {
      "epoch": 7.346938775510204,
      "grad_norm": 4.55275297164917,
      "learning_rate": 3.494667570280132e-05,
      "loss": 1.0155,
      "num_input_tokens_seen": 1149296,
      "step": 405
    },
    {
      "epoch": 7.437641723356009,
      "grad_norm": 6.138458728790283,
      "learning_rate": 3.2804436305991214e-05,
      "loss": 0.932,
      "num_input_tokens_seen": 1163424,
      "step": 410
    },
    {
      "epoch": 7.528344671201814,
      "grad_norm": 0.27833861112594604,
      "learning_rate": 3.071700257475768e-05,
      "loss": 0.9938,
      "num_input_tokens_seen": 1177504,
      "step": 415
    },
    {
      "epoch": 7.619047619047619,
      "grad_norm": 4.685563564300537,
      "learning_rate": 2.8686077049993287e-05,
      "loss": 1.0677,
      "num_input_tokens_seen": 1191632,
      "step": 420
    },
    {
      "epoch": 7.709750566893424,
      "grad_norm": 1.7718054056167603,
      "learning_rate": 2.671331618368682e-05,
      "loss": 0.9402,
      "num_input_tokens_seen": 1205648,
      "step": 425
    },
    {
      "epoch": 7.800453514739229,
      "grad_norm": 0.15285469591617584,
      "learning_rate": 2.4800328987897427e-05,
      "loss": 0.9435,
      "num_input_tokens_seen": 1219632,
      "step": 430
    },
    {
      "epoch": 7.891156462585034,
      "grad_norm": 1.337067723274231,
      "learning_rate": 2.2948675722421086e-05,
      "loss": 1.0292,
      "num_input_tokens_seen": 1233744,
      "step": 435
    },
    {
      "epoch": 7.981859410430839,
      "grad_norm": 1.352471947669983,
      "learning_rate": 2.115986662222058e-05,
      "loss": 0.9199,
      "num_input_tokens_seen": 1247792,
      "step": 440
    },
    {
      "epoch": 8.072562358276643,
      "grad_norm": 0.05873989686369896,
      "learning_rate": 1.943536066565603e-05,
      "loss": 0.9835,
      "num_input_tokens_seen": 1261920,
      "step": 445
    },
    {
      "epoch": 8.16326530612245,
      "grad_norm": 0.10194183886051178,
      "learning_rate": 1.777656438452129e-05,
      "loss": 0.9028,
      "num_input_tokens_seen": 1276224,
      "step": 450
    },
    {
      "epoch": 8.253968253968253,
      "grad_norm": 0.15940436720848083,
      "learning_rate": 1.6184830716856347e-05,
      "loss": 1.0391,
      "num_input_tokens_seen": 1290528,
      "step": 455
    },
    {
      "epoch": 8.344671201814059,
      "grad_norm": 0.10761035233736038,
      "learning_rate": 1.466145790347183e-05,
      "loss": 0.9164,
      "num_input_tokens_seen": 1304800,
      "step": 460
    },
    {
      "epoch": 8.435374149659864,
      "grad_norm": 0.07249670475721359,
      "learning_rate": 1.3207688429084974e-05,
      "loss": 0.9422,
      "num_input_tokens_seen": 1319056,
      "step": 465
    },
    {
      "epoch": 8.526077097505668,
      "grad_norm": 0.05813974142074585,
      "learning_rate": 1.1824708008931418e-05,
      "loss": 0.9493,
      "num_input_tokens_seen": 1333216,
      "step": 470
    },
    {
      "epoch": 8.616780045351474,
      "grad_norm": 0.06978096067905426,
      "learning_rate": 1.051364462167881e-05,
      "loss": 0.9579,
      "num_input_tokens_seen": 1347632,
      "step": 475
    },
    {
      "epoch": 8.70748299319728,
      "grad_norm": 0.07139873504638672,
      "learning_rate": 9.275567589431178e-06,
      "loss": 1.0091,
      "num_input_tokens_seen": 1361696,
      "step": 480
    },
    {
      "epoch": 8.798185941043084,
      "grad_norm": 0.06326282769441605,
      "learning_rate": 8.111486705574534e-06,
      "loss": 0.9624,
      "num_input_tokens_seen": 1375808,
      "step": 485
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 0.0507177971303463,
      "learning_rate": 7.022351411174866e-06,
      "loss": 0.9284,
      "num_input_tokens_seen": 1389776,
      "step": 490
    },
    {
      "epoch": 8.979591836734693,
      "grad_norm": 0.10938501358032227,
      "learning_rate": 6.009050020600459e-06,
      "loss": 0.9439,
      "num_input_tokens_seen": 1403984,
      "step": 495
    },
    {
      "epoch": 9.070294784580499,
      "grad_norm": 0.09296812117099762,
      "learning_rate": 5.072408996999844e-06,
      "loss": 1.0112,
      "num_input_tokens_seen": 1417840,
      "step": 500
    },
    {
      "epoch": 9.160997732426305,
      "grad_norm": 0.09332025796175003,
      "learning_rate": 4.2131922782267405e-06,
      "loss": 0.9769,
      "num_input_tokens_seen": 1431936,
      "step": 505
    },
    {
      "epoch": 9.251700680272108,
      "grad_norm": 0.08040584623813629,
      "learning_rate": 3.4321006537612165e-06,
      "loss": 0.9948,
      "num_input_tokens_seen": 1446160,
      "step": 510
    },
    {
      "epoch": 9.342403628117914,
      "grad_norm": 0.3980367183685303,
      "learning_rate": 2.7297711931358993e-06,
      "loss": 0.8946,
      "num_input_tokens_seen": 1460464,
      "step": 515
    },
    {
      "epoch": 9.433106575963718,
      "grad_norm": 0.4471881687641144,
      "learning_rate": 2.1067767263327933e-06,
      "loss": 0.9869,
      "num_input_tokens_seen": 1475232,
      "step": 520
    },
    {
      "epoch": 9.523809523809524,
      "grad_norm": 0.07744386792182922,
      "learning_rate": 1.5636253765750508e-06,
      "loss": 0.9189,
      "num_input_tokens_seen": 1488992,
      "step": 525
    },
    {
      "epoch": 9.61451247165533,
      "grad_norm": 0.04916306957602501,
      "learning_rate": 1.1007601458942752e-06,
      "loss": 1.0741,
      "num_input_tokens_seen": 1502512,
      "step": 530
    },
    {
      "epoch": 9.705215419501133,
      "grad_norm": 0.06244494393467903,
      "learning_rate": 7.185585538117657e-07,
      "loss": 0.998,
      "num_input_tokens_seen": 1516416,
      "step": 535
    },
    {
      "epoch": 9.795918367346939,
      "grad_norm": 0.05869263783097267,
      "learning_rate": 4.173323294281994e-07,
      "loss": 0.9434,
      "num_input_tokens_seen": 1530384,
      "step": 540
    },
    {
      "epoch": 9.886621315192745,
      "grad_norm": 0.05915815010666847,
      "learning_rate": 1.973271571728441e-07,
      "loss": 0.9795,
      "num_input_tokens_seen": 1544624,
      "step": 545
    },
    {
      "epoch": 9.977324263038549,
      "grad_norm": 0.07499971240758896,
      "learning_rate": 5.872247641987016e-08,
      "loss": 0.9372,
      "num_input_tokens_seen": 1559296,
      "step": 550
    },
    {
      "epoch": 9.977324263038549,
      "num_input_tokens_seen": 1559296,
      "step": 550,
      "total_flos": 7.041059897160499e+16,
      "train_loss": 1.7235560157082297,
      "train_runtime": 1180.2697,
      "train_samples_per_second": 7.473,
      "train_steps_per_second": 0.466
    }
  ],
  "logging_steps": 5,
  "max_steps": 550,
  "num_input_tokens_seen": 1559296,
  "num_train_epochs": 10,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.041059897160499e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
